{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to CNNs\n",
        "Convolutional Neural Networks (CNNs) are a type of deep neural network specifically designed for image processing tasks like classification, detection, and segmentation. CNNs operates on grids of data to capture spatial patterns in images.\n",
        "\n",
        "# Introduction to Convolutional Layers\n",
        "\n",
        "A Convolutional Layer is the fundamental building block of a Convolutional Neural Network (CNN). It is designed to automatically and adaptively learn spatial relationships of features from input images. This is achieved through the process of convolution, where a small filter or kernel is applied to an input image to produce a feature map (or a convolutional output).\n",
        "\n",
        "### Some typical layers found in a CNN:\n",
        "Convolutional Layer: Captures spatial features.\n",
        "ReLU Activation: Introduces non-linearity.\n",
        "Pooling Layer: Reduces dimensionality of convolution outputs, retaining important information.\n",
        "Fully Connected Layer: Makes final predictions. Usually found after all convolution layers.\n",
        "\n",
        "# Convolution Layer Parameters (Hyperparameters):\n",
        "\n",
        "1. Input Image: The input is a multi-dimensional array (usually a 2D image). For grayscale images, the input is a 2D matrix, but for color images, it's 3D (height, width, and color channels like RGB).\n",
        "2. Filter (Kernel): A small matrix that slides (convolves) over the input image. This filter detects specific features like edges, textures, and patterns in the image.\n",
        "3. Stride: The step size by which the filter moves across the input image. Larger strides result in smaller feature maps, reducing dimensionality.\n",
        "4. Padding: To control the size of the output feature map, padding can be applied. Zero-padding adds pixels with zero value around the border, preserving the size of the input after the convolution operation.\n",
        "\n",
        "# How Does Convolution Work?\n",
        "Imagine an input image as a 5x5 matrix and a 3x3 filter (kernel). The convolution operation multiplies the filter's values with the corresponding image region and sums them up to produce a single value in the feature map. The filter \"slides\" over the image to produce a new output image (feature map).\n",
        "\n",
        "### ReLU Activation :\n",
        "After the convolution operation, the output usually goes through a ReLU (Rectified Linear Unit) activation function. This function applies the rule ```ReLU(x)=max(0,x)```, making all negative values zero. This introduces non-linearity, which helps in learning complex patterns.\n",
        "\n",
        "# Example in PyTorch:\n",
        "In the code, we use the following convolutional layers:"
      ],
      "metadata": {
        "id": "vcsZQMYnjTUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of a Convolution Operation\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a 5x5 input (1 channel image, or grayscale)\n",
        "input_image = torch.tensor([\n",
        "    [1.0, 0.0, 1.0, 0.0, 1.0],\n",
        "    [0.0, 1.0, 0.0, 1.0, 0.0],\n",
        "    [1.0, 0.0, 1.0, 0.0, 1.0],\n",
        "    [0.0, 1.0, 0.0, 1.0, 0.0],\n",
        "    [1.0, 0.0, 1.0, 0.0, 1.0]\n",
        "]).unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions (1x1x5x5)\n",
        "\n",
        "# Define a 3x3 kernel\n",
        "kernel = torch.tensor([\n",
        "    [0.0, 1.0, 0.0],\n",
        "    [1.0, 1.0, 1.0],\n",
        "    [0.0, 1.0, 0.0]\n",
        "]).unsqueeze(0).unsqueeze(0)  # Add dimensions to match the input (1x1x3x3)\n",
        "\n",
        "# Create a 2D convolution layer with the kernel (also called filter) as weights\n",
        "conv_layer = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=0, bias=False)\n",
        "# Here we are explicitly making the filter weights as a parameters. This line is not required while implementing as part of a CNN\n",
        "conv_layer.weight = nn.Parameter(kernel)  # Manually set the kernel weights\n",
        "\n",
        "# Apply the convolution to the input image\n",
        "output_image = conv_layer(input_image)\n",
        "\n",
        "print(\"Input Image:\\n\", input_image.squeeze().numpy())\n",
        "print(\"Kernel (Filter):\\n\", kernel.squeeze().numpy())\n",
        "print(\"Output Feature Map after Convolution:\\n\", output_image.squeeze().detach().numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTUYzUkJktST",
        "outputId": "fb564aee-56be-4dc6-d3ce-85600936c3f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Image:\n",
            " [[1. 0. 1. 0. 1.]\n",
            " [0. 1. 0. 1. 0.]\n",
            " [1. 0. 1. 0. 1.]\n",
            " [0. 1. 0. 1. 0.]\n",
            " [1. 0. 1. 0. 1.]]\n",
            "Kernel (Filter):\n",
            " [[0. 1. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 1. 0.]]\n",
            "Output Feature Map after Convolution:\n",
            " [[1. 4. 1.]\n",
            " [4. 1. 4.]\n",
            " [1. 4. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benefits of Convolution:\n",
        "1. Parameter Sharing: Unlike fully connected layers where each neuron has its own weights, convolutional layers use the same filter across the entire image, significantly reducing the number of parameters.\n",
        "2. Local Connectivity: Each filter is applied to a small local region of the input, making it efficient in capturing spatial characteristics, such as edges, textures, and patterns.\n",
        "3. In a CNN, multiple convolutional layers are stacked to capture different levels of information: from low-level features like edges in the first layers to high-level representations like full objects in the deeper layers.\n",
        "\n",
        "An important property of a CNN is the \"receptive field\", which is essentially the portion of the input image that the network sees at different layers. For example, the first layer sees the smallest (local) parts of the image, and as we go deeper into the network, the network sees larger parts of the image (global) because of the successive convolution operations. Try to visualize this yourself. Hint: the first layer of a CNN sees a part of the image equal to the filter size, and as we slide the filter through the entire image, we get an output that sees only small parts of the input image. The next layer sees the output of the first layer, which has already seen portions of the input image, which means that the second layer effectively sees a larger part of the input image determined by the filter sizes of both the first and the second layer.   "
      ],
      "metadata": {
        "id": "4KRYYEV1nAWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation in PyTorch"
      ],
      "metadata": {
        "id": "m6ppE3Ick0Te"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# 1. Load and preprocess data (an example dataset called MNIST)\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "# 2. Define the CNN model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        # Convolutional layer 1\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1) # Single input channel and 32 output channels\n",
        "        # Pooling layer\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # Convolutional layer 2\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1) # 32 input channels and 64 output channels\n",
        "        # Fully connected layers (or dense layers)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        # Activation function\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolution -> ReLU -> Pooling\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        # Flatten the tensor before the fully connected layer\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        # Fully connected layer -> ReLU -> Fully connected layer\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# 3. Instantiate the model, loss function, and optimizer\n",
        "model = SimpleCNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 4. Train the model\n",
        "def train_model():\n",
        "    for epoch in range(5):  # Loop over the dataset 5 times\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in trainloader:\n",
        "            optimizer.zero_grad()  # Zero the parameter gradients\n",
        "            outputs = model(inputs)  # Forward pass\n",
        "            loss = criterion(outputs, labels)  # Compute the loss\n",
        "            loss.backward()  # Backward pass\n",
        "            optimizer.step()  # Optimize\n",
        "\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(trainloader)}\")\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "train_model()\n",
        "\n",
        "# 5. Test the model\n",
        "def test_model():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():  # No need to track gradients during testing\n",
        "        for inputs, labels in testloader:\n",
        "            outputs = model(inputs)  # Forward pass\n",
        "            _, predicted = torch.max(outputs, 1)  # Get predicted label\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f\"Accuracy on test set: {100 * correct / total}%\")\n",
        "\n",
        "test_model()\n"
      ],
      "metadata": {
        "id": "vsKsAJ5fjbnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above example, we used a pre-existing dataset that is offered by PyTorch. However, we can also use our own data in the form of a dataloader that we discussed in the previous tutorial notebook. An example of the same dataloader used previously is shown below. Feel free to modify the code below based on your requirements."
      ],
      "metadata": {
        "id": "rVdHwxwso5BW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# An example of an image dataloader\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "# This inherits from the Dataset subclass\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, img_list, labels, transform=transforms.ToTensor()):\n",
        "        self.img_list = img_list # Input image paths\n",
        "        self.labels = labels # Corresponding ground truth labels\n",
        "        self.transform = transform # Preprocessing transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_list) # Returns the total number of training samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = Image.open(self.img_list[idx]) # Opening the image from its path\n",
        "        if x.mode != 'RGB':\n",
        "            x = x.convert('RGB') # Convert the image to RGB if it's not\n",
        "        x = self.transform(x) # Apply the transform\n",
        "        label = self.labels[idx] # Get the label for the image\n",
        "        return x , label"
      ],
      "metadata": {
        "id": "cK_VTO9rpc89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some Useful Tips for the Assignments\n",
        "\n",
        "1. Since Assignment 1 involves training a DCGAN, you will need to use a network that contains only convolutional layers for the generator. For this, all layers should be of type ```nn.Conv2d()```. Feel free to experiment with different activation functions like  ```nn.ReLU()``` or ```nn.Tanh()```.\n",
        "2. After defining a class for the generator, you will have to define another class for the disriminator, that takes in the same input and outputs a prediction of whether the input is generated or it comes from your dataset. Since the discriminator predicts a class, the final layers must be of type ```nn.Linear()```. Before adding the linear or fully connected layers, you may want to pool the convolutional outputs using ```nn.AvgPool2d()``` to bring it to a lower dimension that is acceptable for a linear layer as an input.   \n",
        "3. The final activation function for the discriminator should be a ```sigmoid``` function so that the output is restricted to 0 to 1. The loss function for training the discriminator could be the binary cross entropy (BCE) loss which can be implemented using ```nn.BCELoss()```.\n",
        "4. Write a training loop for training both the generator and discriminator. Keep in mind that ```model.train()``` and ```model.eval()``` are functions that need to be called in order to choose the right models or parameters that need to be updated in an iteration.\n",
        "5. **Important:** It is highly possible that the GAN you train might not converge properly, so please be patient, and train the models multiple times with different learning rates. Once you find a model that is trained relatively well, you can save the model using the following function and calling it in the training loop after some fixed number of epochs."
      ],
      "metadata": {
        "id": "QldeckvXpgGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Function to save both generator and discriminator models\n",
        "def save_models(generator, discriminator, epoch, folder='saved_models'):\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    torch.save(generator.state_dict(), f\"{folder}/generator_epoch_{epoch}.pth\")\n",
        "    torch.save(discriminator.state_dict(), f\"{folder}/discriminator_epoch_{epoch}.pth\")\n",
        "    print(f\"Models saved at epoch {epoch}.\")\n",
        "\n",
        "# Example of how to save the models at a particular epoch of training\n",
        "save_models(generator, discriminator, epoch=10) # This line should be used in the training loop\n",
        "\n",
        "# Function to load both generator and discriminator models\n",
        "def load_models(generator, discriminator, epoch, folder='saved_models'):\n",
        "    generator.load_state_dict(torch.load(f\"{folder}/generator_epoch_{epoch}.pth\"))\n",
        "    discriminator.load_state_dict(torch.load(f\"{folder}/discriminator_epoch_{epoch}.pth\"))\n",
        "    print(f\"Models loaded from epoch {epoch}.\")\n",
        "\n",
        "# Example of loading the model in case you want to test its generation ability\n",
        "load_models(generator, discriminator, epoch=10)\n"
      ],
      "metadata": {
        "id": "jwdYXDVQu5F7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Happy training!"
      ],
      "metadata": {
        "id": "xyNaug6QvZm2"
      }
    }
  ]
}